# ğŸ¦™ Ollama Chat Stream

A full-stack local AI chat app using **LLaMA 3 (2.1B)** with a **FastAPI** backend and **React/Next.js** frontend â€” streaming responses just like ChatGPT.

> ğŸ’» Everything runs **100% locally**. No API keys. No cloud.

---

## ğŸš€ Features

- ğŸ” Real-time streaming output (like ChatGPT)
- ğŸ§  LLaMA 3 running locally
- âš¡ FastAPI backend with streaming support
- ğŸ’¬ React/Next.js frontend
- ğŸ”Œ Simple setup & easy to extend
- ğŸ›¡ï¸ CORS-enabled backend

---


## ğŸ“¦ Tech Stack

| Layer     | Tech                            |
|-----------|----------------------------------|
| Model     | [LLaMA 3 (2.1B)] via Ollama     |
| Backend   | FastAPI + Uvicorn + Python      |
| Frontend  | React (Next.js) + Fetch stream  |
| Protocol  | `text/event-stream` (NDJSON)    |

---

## ğŸ“‚ Project Structure

ollama-chat-stream/
â”œâ”€â”€ FastAPI-ollama-chat-stream/
â”‚ â”œâ”€â”€ main.py
â”‚ â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ (Next.js app files)
â”œâ”€â”€ README.md


---

## ğŸ§  1. Run Ollama with LLaMA 3

Install and run Ollama:

```bash
brew install ollama
ollama run llama3:2.1b

Ollama will run the model locally on http://localhost:11434

---

## 2. Backend Setup (FastAPI)
cd backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
