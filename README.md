
# ğŸ¦™ FastAPI Llama Chat Stream

A full-stack local AI chat app using **LLaMA 3 (2.1B)** , with a **FastAPI** backend and **React/Next.js** frontend â€” streaming responses just like ChatGPT.

> ğŸ’» Everything runs **100% locally**. No API keys. No cloud.

---

## ğŸš€ Features

- ğŸ” Real-time streaming output (like ChatGPT)
- ğŸ§  LLaMA 3 via Ollama running locally
- âš¡ FastAPI backend with streaming support
- ğŸ’¬ React/Next.js frontend
- ğŸ”Œ Simple setup & easy to extend
- ğŸ›¡ï¸ CORS-enabled backend

---


## ğŸ“¦ Tech Stack

| Layer     | Tech                            |
|-----------|----------------------------------|
| Model     | [LLaMA 3 (2.1B)] via Ollama     |
| Backend   | FastAPI + Uvicorn + Python      |
| Frontend  | React (Next.js) + Fetch stream  |
| Protocol  | `text/event-stream` (NDJSON)    |

---

## ğŸ“‚ Project Structure

```
ollama-chat-stream/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ (Next.js app files)
â”œâ”€â”€ README.md
```

---

## ğŸ§  1. Run Ollama with LLaMA 3

Install and run Ollama:

```bash
brew install ollama
ollama run llama3:2.1b
```

Ollama will run the model locally on `http://localhost:11434`.

---

## ğŸ”§ 2. Backend Setup (FastAPI)

### âœ… Setup

```bash
cd backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### ğŸ“„ `requirements.txt`

```txt
fastapi
uvicorn
requests
```


### â–¶ï¸ Run Backend

```bash
uvicorn main:app --reload --port 8000
```

---

## âš›ï¸ 3. Frontend Setup (React/Next.js)

### âœ… Create App

You can use `create-next-app` or clone from this repo.

```bash
cd ../frontend
npm install
npm run dev
```

Runs on: `http://localhost:3000`


---

## âœ… Sample Interaction

```json
POST http://localhost:8000/api/stream
{
  "prompt": "Explain black holes like I'm 5."
}
```

Response stream:
```json
{"response": "Okay!"}
{"response": " A black hole is..."}
```

---

## ğŸ“Œ Notes

- Ensure Ollama is running before you start the FastAPI backend.
- Use a proxy or CORS middleware for real deployment.
- You can switch `model` in `main.py` to any other Ollama model.

---

## ğŸ“„ License

MIT License â€” use freely and modify for your needs.

---

## ğŸ‘¤ Author

Made with â¤ï¸ by [Odara Rapheal](https://github.com/odara-rapheal)

---

## ğŸŒ Useful Links

- [FastAPI Docs](https://fastapi.tiangolo.com)
- [Next.js Docs](https://nextjs.org/docs)
- [LLaMA 3 Info](https://llama.meta.com/llama3/)
